{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "AIzaSyC4JjUl-n7Rw5DWWLSZzSc8eUHbiLZHq-8\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    " \n",
    "# Load environment variables from the .env file\n",
    "load_dotenv('.env')\n",
    " \n",
    "# Access environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "GOOGLE_API_KEY = 'AIzaSyC4JjUl-n7Rw5DWWLSZzSc8eUHbiLZHq-8'\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-pro',google_api_key = GOOGLE_API_KEY)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model = 'models/embeddings-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class GoogleVertextAI(DeepEvalBaseLLM):\n",
    "    \"\"\"Class to implement Vertex AI for DeepEval\"\"\"\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self,prompt:str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "    \n",
    "    async def a_generate(self,prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return \"Vertex AI Model\"\n",
    "    \n",
    "\n",
    "vertexai_gemini = GoogleVertextAI(model = model)\n",
    "print(vertexai_gemini.generate(\"write me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The score is 1.00 because the output is completely relevant to the input and contains no irrelevant statements.\n"
     ]
    }
   ],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Python or R? what is Better?\",\n",
    "    actual_output=\"Python is best\"\n",
    ")\n",
    "\n",
    "relevancy_metric = AnswerRelevancyMetric(threshold=0.9,model=vertexai_gemini)\n",
    "\n",
    "relevancy_metric.measure(test_case)\n",
    "print(relevancy_metric.score,relevancy_metric.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\TEJKIRAN\\anaconda3\\envs\\deepeval\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The Actual Output is shorter than the Expected Output and meets the specified threshold for shortness.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Python or R? what is Better?\",\n",
    "    actual_output=\"Python is better\",\n",
    "    expected_output=\"Python\"\n",
    ")\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name='ABC',\n",
    "    criteria = \"ABC - determine if output is short or not\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT], model = vertexai_gemini\n",
    ")\n",
    "\n",
    "correctness_metric.measure(test_case)\n",
    "print(correctness_metric.score,correctness_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.scorer import Scorer\n",
    "from deepeval.metrics import BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "class RougeMetric(BaseMetric):\n",
    "    def __init__(self,threshold:float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.scorer = Scorer()\n",
    "\n",
    "    def measure(self, test_case: LLMTestCase):\n",
    "        self.score = self.scorer.rouge_score(\n",
    "            prediction=test_case.actual_output,\n",
    "            target=test_case.expected_output,\n",
    "            score_type=\"rouge1\"\n",
    "        )\n",
    "        self.success = self.score >= self.threshold\n",
    "        return self.score\n",
    "    \n",
    "    async def a_measure(self, test_case: LLMTestCase):\n",
    "        return self.measure(test_case)\n",
    "    \n",
    "    def is_successful(self):\n",
    "        return self.success\n",
    "    \n",
    "    @property\n",
    "    def __name__(self):\n",
    "        return \"Rouge Metric\"\n",
    "    \n",
    "test_case = LLMTestCase(input=\"Is python better than R\",actual_output=\"Yes it is\" , expected_output=\"yes\")\n",
    "metric = RougeMetric()\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.is_successful())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than one testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Vertex AI Model, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Vertex AI Model, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 2 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (2/2) [Time Taken: 00:05,  2.57s/test case]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: Vertex AI Model, reason: The score is 1.00 because there are no irrelevant statements in the actual output., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: who won the IPL 2024\n",
      "  - actual output: KKR\n",
      "  - expected output: KKR won it\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: Vertex AI Model, reason: The answer relevancy score is 1.00 because there are no irrelevant statements in the actual output., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Virat's sirname?\n",
      "  - actual output: Kohli\n",
      "  - expected output: Virat Kohli is the sirname\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8b28e/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8b28e/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">b28e/test-cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! View results on \n",
       "\u001b]8;id=519816;https://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8b28e/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=519816;https://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8b28e/test-cases\u001b\\\u001b[4;94mb28e/test-cases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n",
       "‚ÄºÔ∏è  Friendly reminder üòá: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no irrelevant statements in the actual output.', strict_mode=False, evaluation_model='Vertex AI Model', error=None, evaluation_cost=None, verbose_logs='Statements:\\n[\\n    \"KKR\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='who won the IPL 2024', actual_output='KKR', expected_output='KKR won it', context=None, retrieval_context=None), TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The answer relevancy score is 1.00 because there are no irrelevant statements in the actual output.', strict_mode=False, evaluation_model='Vertex AI Model', error=None, evaluation_cost=None, verbose_logs='Statements:\\n[\\n    \"Kohli\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input=\"What is Virat's sirname?\", actual_output='Kohli', expected_output='Virat Kohli is the sirname', context=None, retrieval_context=None)], confident_link='https://app.confident-ai.com/project/cm456pth10ome11cml3ab9sf5/evaluation-n-testing/test-runs/cm45a20fm0tfc5f8o0xt8b28e/test-cases')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "first_test_case = LLMTestCase(input=\"who won the IPL 2024\",actual_output=\"KKR\" , expected_output=\"KKR won it\")\n",
    "second_test_case = LLMTestCase(input=\"What is Virat's sirname?\",actual_output=\"Kohli\" , expected_output=\"Virat Kohli is the sirname\")\n",
    "\n",
    "test_cases = [first_test_case,second_test_case]\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=test_cases)\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5,model=vertexai_gemini)\n",
    "\n",
    "#dataset.evaluate([answer_relevancy_metric])\n",
    "\n",
    "# we can also call the evaluate() function directly\n",
    "evaluate(dataset,[answer_relevancy_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
